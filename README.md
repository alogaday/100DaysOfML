# 100DaysOfML


Day 1: 2nd February:

Started the streak.

- Had a glance at Andrew ng notes
	- Hypothesis function
		prediction or y hat >> h(theta) = theta0 + theta1 * X1
	- Cost function
		sum of root squared error
		difference between actual value and hypothesis function value
		J(theta0, theta1) = Root mean squared error
	- Gradient descent:
		derivative of cost function with respective to theta, i.e: dJ/dTheta
		update theta values >> theta = theta - alpha * dJ/dTheta

